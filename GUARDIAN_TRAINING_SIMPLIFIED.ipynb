{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed571e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "!pip install -q tensorflow scikit-learn scipy h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'GPU: {\", \".join([d.name for d in tf.config.list_physical_devices(\"GPU\")]) or \"None\"}')\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5bf043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATION\n",
    "print('\\n' + '='*70)\n",
    "print('GENERATING 50,000 SAMPLES')\n",
    "print('='*70)\n",
    "\n",
    "CLASSES = ['blast', 'gunshot', 'artillery', 'vehicle_crash', 'fall', 'normal']\n",
    "FS = 200\n",
    "SEQ_LEN = 200\n",
    "SAMPLES_PER_CLASS = 8333\n",
    "\n",
    "def generate_impact(impact_type, fs=200, seq_len=200):\n",
    "    \"\"\"Generate physics-based impact signature\"\"\"\n",
    "    t = np.linspace(0, seq_len/fs, seq_len)\n",
    "    \n",
    "    # Impact parameters\n",
    "    params = {\n",
    "        'blast': {'amp': (150, 300), 'freq': (50, 150), 'decay': 15, 'sev': (0.6, 1.0)},\n",
    "        'gunshot': {'amp': (40, 100), 'freq': (100, 300), 'decay': 25, 'sev': (0.5, 0.9)},\n",
    "        'artillery': {'amp': (200, 400), 'freq': (30, 100), 'decay': 12, 'sev': (0.6, 1.0)},\n",
    "        'vehicle_crash': {'amp': (20, 80), 'freq': (5, 30), 'decay': 5, 'sev': (0.3, 0.7)},\n",
    "        'fall': {'amp': (15, 60), 'freq': (3, 20), 'decay': 8, 'sev': (0.3, 0.7)},\n",
    "        'normal': {'amp': (0.5, 3), 'freq': (0.1, 5), 'decay': 2, 'sev': (0.0, 0.2)}\n",
    "    }\n",
    "    \n",
    "    p = params[impact_type]\n",
    "    \n",
    "    # Generate signal\n",
    "    amp = np.random.uniform(*p['amp'])\n",
    "    freq = np.random.uniform(*p['freq'])\n",
    "    \n",
    "    # Impact at random position\n",
    "    start = np.random.randint(10, seq_len - 50)\n",
    "    duration = np.random.randint(20, 80)\n",
    "    \n",
    "    signal_wave = np.zeros(seq_len)\n",
    "    impact_t = t[start:start+duration] - t[start]\n",
    "    signal_wave[start:start+duration] = amp * np.sin(2*np.pi*freq*impact_t) * np.exp(-p['decay']*impact_t)\n",
    "    \n",
    "    # Add noise\n",
    "    signal_wave += np.random.normal(0, amp*0.1, seq_len)\n",
    "    \n",
    "    # 3-axis accel (z primary, x,y coupled)\n",
    "    accel_z = signal_wave\n",
    "    accel_x = np.roll(signal_wave, 3) * np.random.uniform(0.2, 0.5) + np.random.normal(0, 0.5, seq_len)\n",
    "    accel_y = np.roll(signal_wave, 5) * np.random.uniform(0.2, 0.5) + np.random.normal(0, 0.5, seq_len)\n",
    "    \n",
    "    # Gyro (derivative of accel)\n",
    "    gyro_x = np.gradient(accel_y) * 10 + np.random.normal(0, 2, seq_len)\n",
    "    gyro_y = np.gradient(accel_x) * 10 + np.random.normal(0, 2, seq_len)\n",
    "    gyro_z = np.gradient(accel_z) * 10 + np.random.normal(0, 2, seq_len)\n",
    "    \n",
    "    # Magnetometer (mostly stable)\n",
    "    mag = np.array([30, 20, -40]) + np.random.normal(0, 3, (seq_len, 3))\n",
    "    \n",
    "    # Vitals (physiological response)\n",
    "    severity = np.random.uniform(*p['sev'])\n",
    "    \n",
    "    hr_base = np.random.uniform(60, 80)\n",
    "    hr = hr_base + severity * 80 * (1 - np.exp(-np.arange(seq_len)/100)) + np.random.normal(0, 2, seq_len)\n",
    "    hr = np.clip(hr, 40, 180)\n",
    "    \n",
    "    spo2_base = np.random.uniform(96, 99)\n",
    "    spo2 = spo2_base - severity * 15 * (1 - np.exp(-np.arange(seq_len)/150)) + np.random.normal(0, 0.5, seq_len)\n",
    "    spo2 = np.clip(spo2, 70, 100)\n",
    "    \n",
    "    br_base = np.random.uniform(12, 16)\n",
    "    br = br_base + severity * 12 * (1 - np.exp(-np.arange(seq_len)/100)) + np.random.normal(0, 1, seq_len)\n",
    "    br = np.clip(br, 8, 35)\n",
    "    \n",
    "    temp_base = np.random.uniform(36.3, 37.0)\n",
    "    temp = temp_base + severity * 0.5 * (1 - np.exp(-np.arange(seq_len)/200)) + np.random.normal(0, 0.1, seq_len)\n",
    "    temp = np.clip(temp, 34, 39)\n",
    "    \n",
    "    # Combine: (200, 13) [accel(3) + gyro(3) + mag(3) + vitals(4)]\n",
    "    sample = np.column_stack([\n",
    "        accel_x, accel_y, accel_z,\n",
    "        gyro_x, gyro_y, gyro_z,\n",
    "        mag,\n",
    "        hr, spo2, br, temp\n",
    "    ])\n",
    "    \n",
    "    return sample.astype(np.float32), severity\n",
    "\n",
    "# Generate dataset\n",
    "X_all = []\n",
    "y_type_all = []\n",
    "y_sev_all = []\n",
    "\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    print(f'Generating {cls}... ', end='')\n",
    "    for _ in range(SAMPLES_PER_CLASS):\n",
    "        sample, sev = generate_impact(cls)\n",
    "        X_all.append(sample)\n",
    "        \n",
    "        label = np.zeros(len(CLASSES))\n",
    "        label[i] = 1\n",
    "        y_type_all.append(label)\n",
    "        y_sev_all.append(sev)\n",
    "    print('Done')\n",
    "\n",
    "X = np.array(X_all, dtype=np.float32)\n",
    "y_type = np.array(y_type_all, dtype=np.float32)\n",
    "y_sev = np.array(y_sev_all, dtype=np.float32)\n",
    "\n",
    "print(f'\\nDataset shape: {X.shape}')\n",
    "print(f'Saving to data/combat_dataset.h5...')\n",
    "\n",
    "with h5py.File('data/combat_dataset.h5', 'w') as f:\n",
    "    f.create_dataset('X', data=X, compression='gzip')\n",
    "    f.create_dataset('y_type', data=y_type, compression='gzip')\n",
    "    f.create_dataset('y_severity', data=y_sev, compression='gzip')\n",
    "\n",
    "print('✓ Dataset ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25874eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND SPLIT DATA\n",
    "print('\\nLoading dataset...')\n",
    "\n",
    "with h5py.File('data/combat_dataset.h5', 'r') as f:\n",
    "    X = f['X'][:]\n",
    "    y_type = f['y_type'][:]\n",
    "    y_sev = f['y_severity'][:]\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "X_train, X_temp, y_type_train, y_type_temp, y_sev_train, y_sev_temp = train_test_split(\n",
    "    X, y_type, y_sev, test_size=0.3, random_state=42, stratify=y_type.argmax(axis=1)\n",
    ")\n",
    "\n",
    "X_val, X_test, y_type_val, y_type_test, y_sev_val, y_sev_test = train_test_split(\n",
    "    X_temp, y_type_temp, y_sev_temp, test_size=0.5, random_state=42, stratify=y_type_temp.argmax(axis=1)\n",
    ")\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc163861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD MODEL\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def build_model():\n",
    "    inputs = layers.Input(shape=(200, 13))\n",
    "    \n",
    "    # CNN feature extraction\n",
    "    x = layers.Conv1D(64, 7, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(128, 5, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = layers.Conv1D(256, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # Bidirectional GRU\n",
    "    x = layers.Bidirectional(layers.GRU(128, return_sequences=True, dropout=0.2))(x)\n",
    "    x = layers.Bidirectional(layers.GRU(64, return_sequences=True, dropout=0.2))(x)\n",
    "    \n",
    "    # Attention\n",
    "    attn = layers.MultiHeadAttention(num_heads=8, key_dim=16)(x, x)\n",
    "    x = layers.Add()([x, attn])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    # Global pooling\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Outputs\n",
    "    impact_type = layers.Dense(6, activation='softmax', name='impact_type')(x)\n",
    "    severity = layers.Dense(1, activation='sigmoid', name='severity')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[impact_type, severity])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001, clipnorm=1.0),\n",
    "        loss={\n",
    "            'impact_type': tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "            'severity': 'mse'\n",
    "        },\n",
    "        loss_weights={'impact_type': 1.0, 'severity': 0.3},\n",
    "        metrics={\n",
    "            'impact_type': ['accuracy'],\n",
    "            'severity': ['mae']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "print('\\n' + '='*70)\n",
    "print('TRAINING MODEL')\n",
    "print('='*70)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_impact_type_accuracy',\n",
    "        patience=25,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'models/best_model.h5',\n",
    "        monitor='val_impact_type_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {'impact_type': y_type_train, 'severity': y_sev_train},\n",
    "    validation_data=(X_val, {'impact_type': y_type_val, 'severity': y_sev_val}),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\n✓ Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3457d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE\n",
    "print('\\n' + '='*70)\n",
    "print('FINAL EVALUATION')\n",
    "print('='*70)\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test,\n",
    "    {'impact_type': y_type_test, 'severity': y_sev_test},\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extract accuracy\n",
    "test_acc = results[3]  # impact_type_accuracy\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'TEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'TARGET: 0.9500 (95.00%)')\n",
    "\n",
    "if test_acc >= 0.95:\n",
    "    print('✓ TARGET ACHIEVED!')\n",
    "else:\n",
    "    print(f'✗ Below target by {(0.95-test_acc)*100:.2f}%')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f207db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TO TFLITE\n",
    "print('\\nConverting to TFLite...')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('models/impact_classifier.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f'✓ TFLite model saved ({len(tflite_model)/1024:.1f} KB)')\n",
    "print(f'  Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD FILES\n",
    "from google.colab import files\n",
    "\n",
    "print('\\nDownloading files...')\n",
    "files.download('models/impact_classifier.tflite')\n",
    "files.download('models/best_model.h5')\n",
    "print('✓ Download complete')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('DEPLOYMENT READY')\n",
    "print('='*70)\n",
    "print('\\nFiles downloaded:')\n",
    "print('1. impact_classifier.tflite (deploy to Raspberry Pi)')\n",
    "print('2. best_model.h5 (for fine-tuning)')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
