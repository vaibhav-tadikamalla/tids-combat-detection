{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "!pip install -q tensorflow scikit-learn scipy h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, Model\n",
    "import os\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'GPU: {\", \".join([d.name for d in tf.config.list_physical_devices(\"GPU\")]) or \"None\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENHANCED DATA GENERATION ===\n",
    "def generate_enhanced_impact(impact_type):\n",
    "    \"\"\"Multi-harmonic physics model\"\"\"\n",
    "    params = {\n",
    "        'blast': {'amp': (150,300), 'f1': (50,150), 'f2': (100,250), 'decay': (12,18), 'sev': (0.6,1.0)},\n",
    "        'gunshot': {'amp': (40,100), 'f1': (100,300), 'f2': (200,600), 'decay': (20,30), 'sev': (0.5,0.9)},\n",
    "        'artillery': {'amp': (200,400), 'f1': (30,100), 'f2': (50,180), 'decay': (10,15), 'sev': (0.6,1.0)},\n",
    "        'vehicle_crash': {'amp': (20,80), 'f1': (5,30), 'f2': (10,50), 'decay': (3,7), 'sev': (0.3,0.7)},\n",
    "        'fall': {'amp': (15,60), 'f1': (3,20), 'f2': (5,35), 'decay': (5,12), 'sev': (0.3,0.7)},\n",
    "        'normal': {'amp': (0.5,3), 'f1': (0.1,5), 'f2': (1,8), 'decay': (1,3), 'sev': (0.0,0.2)}\n",
    "    }\n",
    "    p = params[impact_type]\n",
    "    \n",
    "    # Multi-harmonic signal\n",
    "    t = np.linspace(0, 1, 200)\n",
    "    amp, f1, f2, decay = [np.random.uniform(*p[k]) for k in ['amp','f1','f2','decay']]\n",
    "    start, dur = np.random.randint(10,50), np.random.randint(30,120)\n",
    "    \n",
    "    sig = np.zeros(200)\n",
    "    impact_t = t[start:start+dur] - t[start]\n",
    "    sig[start:start+dur] = amp * (\n",
    "        np.sin(2*np.pi*f1*impact_t) * np.exp(-decay*impact_t) +\n",
    "        0.3 * np.sin(2*np.pi*f2*impact_t) * np.exp(-decay*1.2*impact_t)\n",
    "    ) + np.random.normal(0, amp*0.1, dur)\n",
    "    \n",
    "    # 3-axis IMU\n",
    "    accel_z = sig\n",
    "    accel_x = np.roll(sig, 5) * np.random.uniform(0.2,0.5) + np.random.normal(0, 0.5, 200)\n",
    "    accel_y = np.roll(sig, 3) * np.random.uniform(0.2,0.5) + np.random.normal(0, 0.5, 200)\n",
    "    \n",
    "    gyro_x = np.gradient(accel_y) * 12 + np.random.normal(0, 2, 200)\n",
    "    gyro_y = np.gradient(accel_x) * 12 + np.random.normal(0, 2, 200)\n",
    "    gyro_z = np.gradient(accel_z) * 12 + np.random.normal(0, 2, 200)\n",
    "    \n",
    "    mag = np.tile([30,20,-40], (200,1)) + np.random.normal(0, 3, (200,3))\n",
    "    \n",
    "    # Enhanced vitals\n",
    "    sev = np.random.uniform(*p['sev'])\n",
    "    hr_base = np.random.uniform(60, 80)\n",
    "    hr = hr_base + sev*90*(1-np.exp(-np.arange(200)/100)) + 3*np.sin(2*np.pi*0.25*t) + np.random.normal(0,2,200)\n",
    "    hr = np.clip(hr, 40, 180)\n",
    "    \n",
    "    spo2 = 97 - sev*18*(1-np.exp(-np.arange(200)/180)) + np.random.normal(0,0.5,200)\n",
    "    spo2 = np.clip(spo2, 70, 100)\n",
    "    \n",
    "    br = 14 + sev*15*(1-np.exp(-np.arange(200)/120)) + np.random.normal(0,1,200)\n",
    "    br = np.clip(br, 8, 40)\n",
    "    \n",
    "    temp = 36.6 + (sev*0.5 if sev<0.7 else -sev) * (1-np.exp(-np.arange(200)/250)) + np.random.normal(0,0.08,200)\n",
    "    temp = np.clip(temp, 34, 39)\n",
    "    \n",
    "    return np.column_stack([accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, mag, hr, spo2, br, temp]).astype(np.float32), sev\n",
    "\n",
    "# Generate 60K samples\n",
    "print('Generating 60,000 samples...')\n",
    "CLASSES = ['blast','gunshot','artillery','vehicle_crash','fall','normal']\n",
    "X, y_type, y_sev = [], [], []\n",
    "\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    print(f'{cls}...', end=' ')\n",
    "    for _ in range(10000):\n",
    "        s, sev = generate_enhanced_impact(cls)\n",
    "        X.append(s)\n",
    "        label = np.zeros(6); label[i] = 1\n",
    "        y_type.append(label)\n",
    "        y_sev.append(sev)\n",
    "    print('✓')\n",
    "\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y_type = np.array(y_type, dtype=np.float32)\n",
    "y_sev = np.array(y_sev, dtype=np.float32)\n",
    "\n",
    "print(f'\\nDataset shape: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68383d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "print('Normalizing features...')\n",
    "X_mean, X_std = X.mean(axis=(0,1), keepdims=True), X.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_type_train, y_type_temp, y_sev_train, y_sev_temp = train_test_split(\n",
    "    X, y_type, y_sev, test_size=0.3, random_state=42, stratify=y_type.argmax(1))\n",
    "X_val, X_test, y_type_val, y_type_test, y_sev_val, y_sev_test = train_test_split(\n",
    "    X_temp, y_type_temp, y_sev_temp, test_size=0.5, random_state=42, stratify=y_type_temp.argmax(1))\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebd20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ADVANCED MODEL ===\n",
    "def res_block(x, f, d=1):\n",
    "    s = x\n",
    "    x = layers.Conv1D(f, 3, padding='same', dilation_rate=d)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv1D(f, 3, padding='same', dilation_rate=d)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if s.shape[-1] != f: s = layers.Conv1D(f, 1)(s)\n",
    "    return layers.ReLU()(layers.Add()([s, x]))\n",
    "\n",
    "inputs = layers.Input((200, 13))\n",
    "x = layers.Conv1D(64, 7, padding='same')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# Residual blocks with dilations\n",
    "x = res_block(x, 64, 1)\n",
    "x = res_block(x, 64, 2)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "x = res_block(x, 128, 1)\n",
    "x = res_block(x, 128, 2)\n",
    "x = res_block(x, 128, 4)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "x = res_block(x, 256, 1)\n",
    "x = res_block(x, 256, 2)\n",
    "x = res_block(x, 256, 4)\n",
    "\n",
    "x = res_block(x, 512, 1)\n",
    "x = res_block(x, 512, 2)\n",
    "\n",
    "# Multi-head attention\n",
    "attn = layers.MultiHeadAttention(16, 32)(x, x)\n",
    "x = layers.LayerNormalization()(layers.Add()([x, attn]))\n",
    "\n",
    "# SE block\n",
    "se = layers.GlobalAveragePooling1D()(x)\n",
    "se = layers.Dense(32, activation='relu')(se)\n",
    "se = layers.Dense(512, activation='sigmoid')(se)\n",
    "x = layers.Multiply()([x, layers.Reshape((1, 512))(se)])\n",
    "\n",
    "# Multi-scale pooling\n",
    "avg = layers.GlobalAveragePooling1D()(x)\n",
    "mx = layers.GlobalMaxPooling1D()(x)\n",
    "attn_w = layers.Dense(1, activation='softmax')(x)\n",
    "attn_p = layers.Flatten()(layers.Dot(1)([layers.Permute((2,1))(x), attn_w]))\n",
    "x = layers.concatenate([avg, mx, attn_p])\n",
    "\n",
    "# Deep head\n",
    "x = layers.Dense(768, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "impact = layers.Dense(6, activation='softmax', name='impact_type')(x)\n",
    "severity = layers.Dense(1, activation='sigmoid', name='severity')(x)\n",
    "\n",
    "model = Model(inputs, [impact, severity])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(0.001, weight_decay=0.0001, clipnorm=1.0),\n",
    "    loss={'impact_type': tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1), 'severity': 'mse'},\n",
    "    loss_weights={'impact_type': 1.0, 'severity': 0.2},\n",
    "    metrics={'impact_type': ['accuracy'], 'severity': ['mae']}\n",
    ")\n",
    "\n",
    "print(f'\\nParameters: {sum([np.prod(v.shape) for v in model.trainable_weights]):,}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ee235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN WITH AUGMENTATION ===\n",
    "@tf.function\n",
    "def augment(x):\n",
    "    # Time shift\n",
    "    x = tf.roll(x, tf.random.uniform((), -10, 10, tf.int32), 0)\n",
    "    # Scale\n",
    "    x *= tf.random.uniform((), 0.9, 1.1)\n",
    "    # Noise\n",
    "    x += tf.random.normal(tf.shape(x), 0, 0.03)\n",
    "    return x\n",
    "\n",
    "def data_gen(X, y_type, y_sev, batch=32, aug=False):\n",
    "    while True:\n",
    "        idx = np.random.permutation(len(X))\n",
    "        for i in range(0, len(X), batch):\n",
    "            batch_idx = idx[i:i+batch]\n",
    "            X_b = X[batch_idx]\n",
    "            if aug:\n",
    "                X_b = np.array([augment(x).numpy() for x in X_b])\n",
    "            yield X_b, {'impact_type': y_type[batch_idx], 'severity': y_sev[batch_idx]}\n",
    "\n",
    "train_gen = data_gen(X_train, y_type_train, y_sev_train, aug=True)\n",
    "val_gen = data_gen(X_val, y_type_val, y_sev_val, aug=False)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping('val_impact_type_accuracy', patience=30, restore_best_weights=True, mode='max'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau('val_loss', 0.5, patience=12, min_lr=1e-7),\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model.h5', 'val_impact_type_accuracy', save_best_only=True, mode='max'),\n",
    "    tf.keras.callbacks.LearningRateScheduler(lambda e: 0.001 * (np.cos(np.pi*e/200)+1)/2)\n",
    "]\n",
    "\n",
    "print('\\nTraining...')\n",
    "print('='*70)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen, steps_per_epoch=len(X_train)//32,\n",
    "    validation_data=val_gen, validation_steps=len(X_val)//32,\n",
    "    epochs=200, callbacks=callbacks, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATE ===\n",
    "print('\\n' + '='*70)\n",
    "print('FINAL EVALUATION')\n",
    "print('='*70)\n",
    "\n",
    "results = model.evaluate(X_test, {'impact_type': y_type_test, 'severity': y_sev_test}, verbose=1)\n",
    "test_acc = results[3]\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'TEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'TARGET: 0.9600 (96.00%)')\n",
    "if test_acc >= 0.96: \n",
    "    print('✓ TARGET ACHIEVED!')\n",
    "else: \n",
    "    print(f'✗ Below by {(0.96-test_acc)*100:.2f}%')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b96c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONVERT TO TFLITE ===\n",
    "print('\\nConverting to TFLite...')\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "try:\n",
    "    tflite_model = converter.convert()\n",
    "    with open('impact_classifier.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f'✓ TFLite saved ({len(tflite_model)/1024:.1f} KB, accuracy: {test_acc:.4f})')\n",
    "except Exception as e:\n",
    "    print(f'⚠ TFLite conversion failed: {e}')\n",
    "    model.save('model.h5')\n",
    "    print('✓ Saved as model.h5 instead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5369ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD FILES ===\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    if os.path.exists('impact_classifier.tflite'):\n",
    "        files.download('impact_classifier.tflite')\n",
    "        print('✓ Downloaded impact_classifier.tflite')\n",
    "    elif os.path.exists('model.h5'):\n",
    "        files.download('model.h5')\n",
    "        print('✓ Downloaded model.h5')\n",
    "    \n",
    "    if os.path.exists('best_model.h5'):\n",
    "        files.download('best_model.h5')\n",
    "        print('✓ Downloaded best_model.h5')\n",
    "    \n",
    "    print('\\n' + '='*70)\n",
    "    print('DEPLOYMENT READY')\n",
    "    print('='*70)\n",
    "except:\n",
    "    print('Not running in Colab - files saved locally')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
