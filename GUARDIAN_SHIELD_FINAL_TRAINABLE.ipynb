{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow scikit-learn scipy h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, Model\n",
    "import os\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'GPU: {tf.config.list_physical_devices(\"GPU\")}')\n",
    "\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED DATA GENERATION (60K samples)\n",
    "print('\\nGenerating 60,000 samples...')\n",
    "\n",
    "CLASSES = ['blast', 'gunshot', 'artillery', 'vehicle_crash', 'fall', 'normal']\n",
    "\n",
    "def generate_sample(impact_type):\n",
    "    params = {\n",
    "        'blast': {'amp': (150,300), 'freq': (50,150), 'decay': 15, 'sev': (0.6,1.0)},\n",
    "        'gunshot': {'amp': (40,100), 'freq': (100,300), 'decay': 25, 'sev': (0.5,0.9)},\n",
    "        'artillery': {'amp': (200,400), 'freq': (30,100), 'decay': 12, 'sev': (0.6,1.0)},\n",
    "        'vehicle_crash': {'amp': (20,80), 'freq': (5,30), 'decay': 5, 'sev': (0.3,0.7)},\n",
    "        'fall': {'amp': (15,60), 'freq': (3,20), 'decay': 8, 'sev': (0.3,0.7)},\n",
    "        'normal': {'amp': (0.5,3), 'freq': (0.1,5), 'decay': 2, 'sev': (0.0,0.2)}\n",
    "    }\n",
    "    \n",
    "    p = params[impact_type]\n",
    "    t = np.linspace(0, 1, 200)\n",
    "    \n",
    "    # Multi-frequency components\n",
    "    amp = np.random.uniform(*p['amp'])\n",
    "    f1 = np.random.uniform(*p['freq'])\n",
    "    f2 = f1 * np.random.uniform(1.3, 1.8)\n",
    "    f3 = f1 * np.random.uniform(0.5, 0.8)\n",
    "    \n",
    "    start = np.random.randint(10, 50)\n",
    "    dur = np.random.randint(30, 120)\n",
    "    \n",
    "    sig = np.zeros(200)\n",
    "    impact_t = t[start:start+dur] - t[start]\n",
    "    \n",
    "    # Multi-harmonic signal\n",
    "    sig[start:start+dur] = (\n",
    "        amp * np.sin(2*np.pi*f1*impact_t) * np.exp(-p['decay']*impact_t) +\n",
    "        amp * 0.3 * np.sin(2*np.pi*f2*impact_t) * np.exp(-p['decay']*impact_t*1.2) +\n",
    "        amp * 0.2 * np.sin(2*np.pi*f3*impact_t) * np.exp(-p['decay']*impact_t*0.8)\n",
    "    )\n",
    "    \n",
    "    sig += np.random.normal(0, amp*0.12, 200)\n",
    "    \n",
    "    # 3-axis IMU\n",
    "    accel_z = sig\n",
    "    accel_x = np.roll(sig, np.random.randint(3,8)) * np.random.uniform(0.2,0.6) + np.random.normal(0, 0.5, 200)\n",
    "    accel_y = np.roll(sig, np.random.randint(3,8)) * np.random.uniform(0.2,0.6) + np.random.normal(0, 0.5, 200)\n",
    "    \n",
    "    gyro_x = np.gradient(accel_y) * np.random.uniform(10,15) + np.random.normal(0, 2, 200)\n",
    "    gyro_y = np.gradient(accel_x) * np.random.uniform(10,15) + np.random.normal(0, 2, 200)\n",
    "    gyro_z = np.gradient(accel_z) * np.random.uniform(10,15) + np.random.normal(0, 2, 200)\n",
    "    \n",
    "    mag = np.tile([30, 20, -40], (200, 1)) + np.random.normal(0, 3, (200, 3))\n",
    "    \n",
    "    # Vitals\n",
    "    sev = np.random.uniform(*p['sev'])\n",
    "    \n",
    "    hr_base = np.random.uniform(60, 80)\n",
    "    hr = hr_base + sev*85*(1-np.exp(-np.arange(200)/100)) + 3*np.sin(2*np.pi*0.25*t) + np.random.normal(0, 2, 200)\n",
    "    hr = np.clip(hr, 40, 180)\n",
    "    \n",
    "    spo2 = 97 - sev*18*(1-np.exp(-np.arange(200)/180)) + np.random.normal(0, 0.5, 200)\n",
    "    spo2 = np.clip(spo2, 70, 100)\n",
    "    \n",
    "    br = 14 + sev*14*(1-np.exp(-np.arange(200)/120)) + np.random.normal(0, 1, 200)\n",
    "    br = np.clip(br, 8, 40)\n",
    "    \n",
    "    temp = 36.6 + sev*0.6*(1-np.exp(-np.arange(200)/250)) + np.random.normal(0, 0.08, 200)\n",
    "    temp = np.clip(temp, 34, 39)\n",
    "    \n",
    "    sample = np.column_stack([accel_x, accel_y, accel_z, gyro_x, gyro_y, gyro_z, mag, hr, spo2, br, temp])\n",
    "    return sample.astype(np.float32), sev\n",
    "\n",
    "# Generate dataset\n",
    "X_all = []\n",
    "y_type_all = []\n",
    "y_sev_all = []\n",
    "\n",
    "for i, cls in enumerate(CLASSES):\n",
    "    print(f'[{i+1}/6] {cls}... ', end='')\n",
    "    for j in range(10000):\n",
    "        if (j+1) % 2000 == 0:\n",
    "            print(f'{j+1}... ', end='')\n",
    "        sample, sev = generate_sample(cls)\n",
    "        X_all.append(sample)\n",
    "        \n",
    "        label = np.zeros(6)\n",
    "        label[i] = 1\n",
    "        y_type_all.append(label)\n",
    "        y_sev_all.append(sev)\n",
    "    print('Done')\n",
    "\n",
    "X = np.array(X_all, dtype=np.float32)\n",
    "y_type = np.array(y_type_all, dtype=np.float32)\n",
    "y_sev = np.array(y_sev_all, dtype=np.float32)\n",
    "\n",
    "print(f'\\nDataset: {X.shape}')\n",
    "print('Normalizing...')\n",
    "\n",
    "# Normalize\n",
    "X_mean = X.mean(axis=(0,1), keepdims=True)\n",
    "X_std = X.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "np.savez('models/norm.npz', mean=X_mean, std=X_std)\n",
    "print('✓ Data ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT DATA\n",
    "X_train, X_temp, y_type_train, y_type_temp, y_sev_train, y_sev_temp = train_test_split(\n",
    "    X, y_type, y_sev, test_size=0.3, random_state=42, stratify=y_type.argmax(1)\n",
    ")\n",
    "\n",
    "X_val, X_test, y_type_val, y_type_test, y_sev_val, y_sev_test = train_test_split(\n",
    "    X_temp, y_type_temp, y_sev_temp, test_size=0.5, random_state=42, stratify=y_type_temp.argmax(1)\n",
    ")\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e713945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE DATA AUGMENTATION\n",
    "def augment_data(X, y_type, y_sev):\n",
    "    \"\"\"Simple augmentation - tested and works\"\"\"\n",
    "    X_aug = []\n",
    "    y_type_aug = []\n",
    "    y_sev_aug = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        # Original\n",
    "        X_aug.append(X[i])\n",
    "        y_type_aug.append(y_type[i])\n",
    "        y_sev_aug.append(y_sev[i])\n",
    "        \n",
    "        # Augmented (50% of samples)\n",
    "        if i % 2 == 0:\n",
    "            x_new = X[i].copy()\n",
    "            \n",
    "            # Time shift\n",
    "            shift = np.random.randint(-8, 8)\n",
    "            x_new = np.roll(x_new, shift, axis=0)\n",
    "            \n",
    "            # Scale\n",
    "            scale = np.random.uniform(0.92, 1.08)\n",
    "            x_new *= scale\n",
    "            \n",
    "            # Noise\n",
    "            x_new += np.random.normal(0, 0.02, x_new.shape)\n",
    "            \n",
    "            X_aug.append(x_new)\n",
    "            y_type_aug.append(y_type[i])\n",
    "            y_sev_aug.append(y_sev[i])\n",
    "    \n",
    "    return np.array(X_aug), np.array(y_type_aug), np.array(y_sev_aug)\n",
    "\n",
    "print('Augmenting training data...')\n",
    "X_train_aug, y_type_train_aug, y_sev_train_aug = augment_data(X_train, y_type_train, y_sev_train)\n",
    "print(f'Augmented: {len(X_train)} → {len(X_train_aug)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08faad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED MODEL\n",
    "def residual_block(x, filters, dilation=1):\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.Conv1D(filters, 3, padding='same', dilation_rate=dilation)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    x = layers.Conv1D(filters, 3, padding='same', dilation_rate=dilation)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1)(shortcut)\n",
    "    \n",
    "    x = layers.Add()([shortcut, x])\n",
    "    return layers.ReLU()(x)\n",
    "\n",
    "print('\\nBuilding model...')\n",
    "\n",
    "inputs = layers.Input(shape=(200, 13))\n",
    "\n",
    "# Initial conv\n",
    "x = layers.Conv1D(64, 7, padding='same')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "\n",
    "# Stage 1\n",
    "x = residual_block(x, 64, dilation=1)\n",
    "x = residual_block(x, 64, dilation=2)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "# Stage 2\n",
    "x = residual_block(x, 128, dilation=1)\n",
    "x = residual_block(x, 128, dilation=2)\n",
    "x = residual_block(x, 128, dilation=4)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "# Stage 3\n",
    "x = residual_block(x, 256, dilation=1)\n",
    "x = residual_block(x, 256, dilation=2)\n",
    "x = residual_block(x, 256, dilation=4)\n",
    "\n",
    "# Stage 4\n",
    "x = residual_block(x, 512, dilation=1)\n",
    "x = residual_block(x, 512, dilation=2)\n",
    "\n",
    "# Attention\n",
    "attn = layers.MultiHeadAttention(num_heads=16, key_dim=32)(x, x)\n",
    "x = layers.Add()([x, attn])\n",
    "x = layers.LayerNormalization()(x)\n",
    "\n",
    "# SE block\n",
    "se = layers.GlobalAveragePooling1D()(x)\n",
    "se = layers.Dense(32, activation='relu')(se)\n",
    "se = layers.Dense(512, activation='sigmoid')(se)\n",
    "se = layers.Reshape((1, 512))(se)\n",
    "x = layers.Multiply()([x, se])\n",
    "\n",
    "# Pooling\n",
    "avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.concatenate([avg_pool, max_pool])\n",
    "\n",
    "# Classification head\n",
    "x = layers.Dense(768, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "# Outputs\n",
    "impact_type = layers.Dense(6, activation='softmax', name='impact_type')(x)\n",
    "severity = layers.Dense(1, activation='sigmoid', name='severity')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[impact_type, severity])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001, clipnorm=1.0),\n",
    "    loss={\n",
    "        'impact_type': tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "        'severity': 'mse'\n",
    "    },\n",
    "    loss_weights={'impact_type': 1.0, 'severity': 0.2},\n",
    "    metrics={'impact_type': ['accuracy'], 'severity': ['mae']}\n",
    ")\n",
    "\n",
    "params = sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "print(f'Parameters: {params:,}')\n",
    "print('✓ Model ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "print('\\n' + '='*70)\n",
    "print('TRAINING')\n",
    "print('='*70)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_impact_type_accuracy',\n",
    "        patience=30,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=12,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'models/best_model.h5',\n",
    "        monitor='val_impact_type_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_aug,\n",
    "    {'impact_type': y_type_train_aug, 'severity': y_sev_train_aug},\n",
    "    validation_data=(X_val, {'impact_type': y_type_val, 'severity': y_sev_val}),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print('\\n✓ Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d48c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE\n",
    "print('\\n' + '='*70)\n",
    "print('FINAL EVALUATION')\n",
    "print('='*70)\n",
    "\n",
    "results = model.evaluate(\n",
    "    X_test,\n",
    "    {'impact_type': y_type_test, 'severity': y_sev_test},\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "test_acc = results[3]  # impact_type_accuracy\n",
    "\n",
    "print(f'\\nTEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'TARGET: 0.9500 (95.00%)')\n",
    "\n",
    "if test_acc >= 0.95:\n",
    "    print('✓ TARGET ACHIEVED!')\n",
    "else:\n",
    "    print(f'✗ Below target by {(0.95-test_acc)*100:.2f}%')\n",
    "\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "print('\\nSaving model...')\n",
    "\n",
    "model.save('models/final_model.h5')\n",
    "print('✓ Saved: models/final_model.h5')\n",
    "\n",
    "# Try TFLite conversion (might fail, but we have .h5 backup)\n",
    "try:\n",
    "    print('\\nAttempting TFLite conversion...')\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open('models/impact_classifier.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f'✓ TFLite saved ({len(tflite_model)/1024:.1f} KB)')\n",
    "except Exception as e:\n",
    "    print(f'⚠ TFLite conversion failed (expected): {str(e)[:100]}')\n",
    "    print('  Use the .h5 model instead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD\n",
    "from google.colab import files\n",
    "\n",
    "print('\\nDownloading files...')\n",
    "\n",
    "# Download whichever model exists\n",
    "if os.path.exists('models/impact_classifier.tflite'):\n",
    "    files.download('models/impact_classifier.tflite')\n",
    "    print('✓ Downloaded: impact_classifier.tflite')\n",
    "else:\n",
    "    files.download('models/final_model.h5')\n",
    "    print('✓ Downloaded: final_model.h5')\n",
    "\n",
    "files.download('models/norm.npz')\n",
    "print('✓ Downloaded: norm.npz (normalization parameters)')\n",
    "\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'FINAL ACCURACY: {test_acc*100:.2f}%')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
